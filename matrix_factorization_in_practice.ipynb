{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SimEc for recommender systems\n",
    "Tasks like product recommendation or drug-target interaction prediction essentially consist of having to predict missing entries in a large matrix containing pairwise relations, e.g., the user ratings of some items or whether or not a drug interacts with a certain protein. Besides the sparse matrix containing the pairwise relations, generally one can also construct some feature vectors for the items and users (drugs / proteins), e.g., based on textual descriptions. These can come in especially handy when predictions need to be made, e.g., for new items that did not receive any user ratings so far. In the following we will only talk about items and users but of course this extends to other problem setups as well like drug-target interaction prediction. We distinguish between 3 tasks with increasing difficulty:\n",
    "- **T1**: Predict missing ratings for existing items and users\n",
    "- **T2a** and **T2b**: Predict ratings for new items and existing users (a) or new users and existing items (b)\n",
    "- **T3**: Predict ratings for new items and new users\n",
    "\n",
    "For tasks T2a/b and T3, feature vectors describing items and/or users are required. \n",
    "\n",
    "In this notebook, we discuss some methods which can be used to solve some or all of the above tasks. These include:\n",
    "##### Baseline Methods\n",
    "- **Predict average**: This is a no-brainer: simply fill all the missing values by averages. For example, an item rating from a user can be predicted based on the average rating the user usually gives (he might in general be more or less critical than other users) and the average rating the item got from other users (it might be better or worse than the average item) or for new items and users just predict the overall average rating (solves **T1, T2a/b, T3**).\n",
    "- **SVD of the ratings matrix**: By factorizing the ratings matrix using (iterative) singular value decomposition (SVD), one can compute a low rank approximation of the ratings matrix and use these approximate values as predictions for the missing values (solves **T1**). This can also be combined with the average ratings from above, i.e., the low rank approximation can be used to predict the residuals.\n",
    "- **SVD + Regression**: Given some feature vectors for items or users and the low rank approximation of the ratings matrix computed above, using a regression model, the mapping from the items' feature vectors to their rating vectors can be learned (or respectively for users). This is an extension of the above method to additionally solve either **T2a** or **T2b**.\n",
    "- **Regression/Classification model**: This approach is completely different from the so-called latent factor models discussed above. Here we train an ordinary regression or classification model (depending on the form of the pairwise data, e.g. continuous ratings or binary interactions) by using as input the concatenation of the feature vectors of an item and a user and as the target their rating. This approach can be used to solve all tasks **T1, T2a/b, T3** provided corresponding feature vectors are available.\n",
    "\n",
    "##### Similarity Encoder Models\n",
    "- **Factorization of the ratings matrix using the identity matrix as input**: By training a SimEc to factorize the ratings matrix using the identity matrix as input, we can recreate the solution obtained with SVD (while possibly better handling missing values when computing the decomposition). Correspondingly, this only solves **T1**.\n",
    "- **Factorization of the ratings matrix using feature vectors as input**: By using either item or user feature vectors as input when factorizing the ratings matrix, we can additionally solve **T2a** or **T2b**.\n",
    "- **Train a second SimEc with feature vectors and fixed last layer weights**: After training, e.g., a SimEc with item feature vectors as input to decompose the ratings matrix, we can use this SimEc to compute the item embeddings $Y$. We can then construct a second SimEc, which uses user feature vectors as input to factorize the ratings matrix. However, here we fix the weights of the last layer by setting them to the transpose of the embedding matrix computed for the items. After this SimEc is trained, we can now use both SimEcs to compute item and user embeddings respectively and then compute the scalar product of the embedding vectors to predict the ratings. This approach can then also be used to predict the ratings given the feature vectors for new items and users, i.e., it can be used to solve all tasks **T1, T2a/b, T3**.\n",
    "\n",
    "If the rating matrix contains explicit ratings (i.e. likes and dislikes), all available entries can be used to train the above models. If the pairwise relations in the matrix only represent implicit feedback or binary interactions (e.g. the user listens to music by certain artists, which means he likes them, but we don't know if he doesn't listen to other artists because he doesn't know them or because he doesn't like them), then we can use the given entries in the matrix as positive examples and additionally take a random sample of the missing entries and use them as negative examples. In the latter case, it might be more useful to use classification instead of regression models and also when training the SimEc it could be helpful to apply a non-linearity on the output before computing the error of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T23:54:21.878134Z",
     "start_time": "2018-03-30T23:54:20.817990Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "np.random.seed(28)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(28)\n",
    "import keras\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from simec import SimilarityEncoder\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T23:54:21.922470Z",
     "start_time": "2018-03-30T23:54:21.879955Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data/recsys/tmdb_data\"):\n",
    "    os.mkdir(\"data/recsys/tmdb_data\")\n",
    "\n",
    "def parse_tmdb(tmdbid, apikey):\n",
    "    movie_data = {}\n",
    "    if os.path.exists(\"data/recsys/tmdb_data/%r.json\" % tmdbid):\n",
    "        with open(\"data/recsys/tmdb_data/%r.json\" % tmdbid) as f:\n",
    "            movie_data = json.load(f)\n",
    "    # for a movie with tmdbid get:\n",
    "    # genres:name, original language en y/n, id, title, overview, release_date-->year,\n",
    "    # keywords:keywords:name, credits:cast:name[:10], credits:crew:(\"job\": \"Director\"):name\n",
    "    if not movie_data:\n",
    "        r = requests.get(\"https://api.themoviedb.org/3/movie/%r?api_key=%s&language=en-US&append_to_response=keywords,credits\" % (tmdbid, apikey))\n",
    "        if r.status_code != 200:\n",
    "            print(\"something went wrong when accessing tmdb with id %r!\" % tmdbid)\n",
    "            print(r.text)\n",
    "        else:\n",
    "            movie_json = r.json()\n",
    "            movie_data['tmdbid'] = movie_json['id']\n",
    "            movie_data['title'] = movie_json['title']\n",
    "            movie_data['overview'] = movie_json['overview']\n",
    "            movie_data['release_date'] = movie_json['release_date']\n",
    "            movie_data['year'] = movie_json[\"release_date\"].split(\"-\")[0]\n",
    "            movie_data['original_en'] = str(movie_json['original_language'] == \"en\")\n",
    "            movie_data['genres'] = [g[\"name\"] for g in movie_json['genres']]\n",
    "            movie_data['keywords'] = [k[\"name\"] for k in movie_json['keywords']['keywords']]\n",
    "            movie_data['cast'] = [c[\"name\"] for c in movie_json['credits']['cast'][:10]]\n",
    "            movie_data['directors'] = [c[\"name\"] for c in movie_json['credits']['crew'] if c[\"job\"] == \"Director\"]\n",
    "            print(\"got data for %s\" % movie_json['title'])\n",
    "            with open(\"data/recsys/tmdb_data/%r.json\" % tmdbid, \"w\") as f:\n",
    "                json.dump(movie_data, f, indent=2)\n",
    "    return movie_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T23:54:22.234384Z",
     "start_time": "2018-03-30T23:54:21.924139Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/franzi/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got data for 10608 movies\n"
     ]
    }
   ],
   "source": [
    "# get movielens data from: https://grouplens.org/datasets/movielens/10m/\n",
    "# load all possible movies (in the 10m dataset)\n",
    "df_movies = pd.read_csv(\"data/recsys/ml-10M100K/movies.dat\", sep=\"::\", names=[\"movieId\",\"title\",\"genres\"])\n",
    "# get corresponding tmdbids (only in 20m dataset)\n",
    "df_links = pd.read_csv(\"data/recsys/ml-20m/links.csv\")\n",
    "df_links = df_links.dropna()\n",
    "df_links = df_links.astype(int) \n",
    "map_movieids = dict(zip(df_links.movieId, df_links.tmdbId))\n",
    "# get additional details from themoviedb.org (assumes api key is stored at data/recsys/tmdb_apikey.txt)\n",
    "if os.path.exists(\"data/recsys/tmdb_data.json\"):\n",
    "    with open(\"data/recsys/tmdb_data.json\") as f:\n",
    "        movies_data = json.load(f)\n",
    "else:\n",
    "    movies_data = {}\n",
    "    with open('data/recsys/tmdb_apikey.txt') as f:\n",
    "        apikey = f.read().strip()\n",
    "    for movieid in df_movies.movieId:\n",
    "        if movieid in map_movieids:\n",
    "            m = parse_tmdb(map_movieids[movieid], apikey)\n",
    "            if m:\n",
    "                # careful: when loading the json later the ids will be strings as well anyways\n",
    "                movies_data[str(movieid)] = m\n",
    "            else:\n",
    "                print(\"error with movie id: %i\" % movieid)\n",
    "    with open(\"data/recsys/tmdb_data.json\", \"w\") as f:\n",
    "        json.dump(movies_data, f, indent=2)\n",
    "print(\"got data for %i movies\" % len(movies_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T23:55:01.424935Z",
     "start_time": "2018-03-30T23:54:22.465419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed 0 lines\n",
      "parsed 1000000 lines\n",
      "parsed 2000000 lines\n",
      "parsed 3000000 lines\n",
      "parsed 4000000 lines\n",
      "parsed 5000000 lines\n",
      "parsed 6000000 lines\n",
      "parsed 7000000 lines\n",
      "parsed 8000000 lines\n",
      "parsed 9000000 lines\n",
      "parsed 10000000 lines\n"
     ]
    }
   ],
   "source": [
    "# load pairwise data and generate a dict with {(userid, movieid): rating}\n",
    "movieids = set()\n",
    "userids = set()\n",
    "tuple_ratings = {}\n",
    "rating_pairs = []\n",
    "with open(\"data/recsys/ml-10M100K/ratings.dat\") as f:\n",
    "    for i, l in enumerate(f.readlines()):\n",
    "        if not i % 1000000:\n",
    "            print(\"parsed %i lines\" % i)\n",
    "        u, m, r, t = l.strip().split(\"::\")\n",
    "        # only consider ratings for movies where we have external data available\n",
    "        if m in movies_data:\n",
    "            # in addition to the ratings, also get a list of all users and movies\n",
    "            if u not in userids:\n",
    "                userids.add(u)\n",
    "            if m not in movieids:\n",
    "                movieids.add(m)\n",
    "            tuple_ratings[(u,m)] = float(r)\n",
    "            rating_pairs.append((u,m))\n",
    "        #else:\n",
    "        #    print(\"warning, skipping rating for movie with id %r\" % m)\n",
    "# shuffle all movie and user ids (important so we can split data into train and test sets)\n",
    "# this list additionally functions as a mapping from a (matrix) index to the actual id\n",
    "np.random.seed(13)\n",
    "map_index2movieid = np.random.permutation(sorted(movieids))\n",
    "map_index2userid = np.random.permutation(sorted(userids))\n",
    "# also get a shuffeled list of all rating pairs\n",
    "rating_pairs = np.random.permutation(rating_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T00:24:39.281416Z",
     "start_time": "2018-03-31T00:24:39.222556Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we have different scenarios: either we're only missing some individual ratings or entire movies/users\n",
    "def split_traintest(scenario):\n",
    "    print(\"generating train/test splits for scenario %r\" % scenario)\n",
    "    if scenario == \"T1\":\n",
    "        # missing ratings\n",
    "        rating_pairs_train = rating_pairs[:int(0.7*len(rating_pairs))]\n",
    "        rating_pairs_test = rating_pairs[int(0.7*len(rating_pairs)):]\n",
    "        map_index2movieid_train = map_index2movieid\n",
    "        map_index2userid_train = map_index2userid\n",
    "    else:\n",
    "        rating_pairs_train = []\n",
    "        rating_pairs_test = []\n",
    "        if scenario == \"T2a\":\n",
    "            # missing movies\n",
    "            map_index2movieid_train = map_index2movieid[:int(0.7*len(map_index2movieid))]\n",
    "            map_index2userid_train = map_index2userid\n",
    "        elif scenario == \"T2b\":\n",
    "            # missing users\n",
    "            map_index2movieid_train = map_index2movieid\n",
    "            map_index2userid_train = map_index2userid[:int(0.7*len(map_index2userid))]\n",
    "        elif scenario == \"T3\":\n",
    "            # missing movies and users\n",
    "            map_index2movieid_train = map_index2movieid[:int(0.85*len(map_index2movieid))]\n",
    "            map_index2userid_train = map_index2userid[:int(0.8*len(map_index2userid))]\n",
    "        else:\n",
    "            raise Exception(\"unknown scenario %r, use either T1, T2a, T2b, or T3!\" % scenario)\n",
    "        movieids_train_set = set(map_index2movieid_train)\n",
    "        userids_train_set = set(map_index2userid_train)\n",
    "        rating_pairs_train = []\n",
    "        rating_pairs_test = []\n",
    "        for (u, m) in rating_pairs:\n",
    "            if u in userids_train_set and m in movieids_train_set:\n",
    "                rating_pairs_train.append((u, m))\n",
    "            else:\n",
    "                rating_pairs_test.append((u, m))\n",
    "    print(\"got %i training and %i test ratings\" % (len(rating_pairs_train), len(rating_pairs_test)))\n",
    "    # create mappings from the actual id to the index\n",
    "    map_movieid2index_train = {m: i for i, m in enumerate(map_index2movieid_train)}\n",
    "    map_userid2index_train = {u: i for i, u in enumerate(map_index2userid_train)}\n",
    "    return rating_pairs_train, rating_pairs_test, map_index2userid_train,\\\n",
    "           map_index2movieid_train, map_userid2index_train, map_movieid2index_train\n",
    "\n",
    "def make_train_matrix(tuple_ratings, rating_pairs_train, map_userid2index_train, map_movieid2index_train):\n",
    "    # transform training ratings into a sparse matrix for convenience\n",
    "    print(\"transforming dict with %i ratings into sparse matrix\" % len(rating_pairs_train))\n",
    "    ratings_matrix = lil_matrix((len(map_userid2index_train),len(map_movieid2index_train)))\n",
    "    for (u, m) in rating_pairs_train:\n",
    "        ratings_matrix[map_userid2index_train[u], map_movieid2index_train[m]] = tuple_ratings[(u, m)]\n",
    "    ratings_matrix = csr_matrix(ratings_matrix)\n",
    "    return ratings_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T00:38:04.285458Z",
     "start_time": "2018-03-31T00:38:04.241348Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MeansModel():\n",
    "    \"\"\"\n",
    "    A very simple baseline model, which predicts the rating a user would give to a movie as:\n",
    "        mean + user_mean + movie_mean\n",
    "    \"\"\"\n",
    "    def __init__(self, shrinkage=1.):\n",
    "        self.mean = None\n",
    "        self.mean_users = {}\n",
    "        self.mean_movies = {}\n",
    "        # shrinkage decreases the influence of the individual user/movie means\n",
    "        # --> mean + shrinkage*user_mean + shrinkage*movie_mean\n",
    "        # it should always be between 0 and 1; 0 means individual means are ignored\n",
    "        self.shrinkage = max(0., min(1., shrinkage))\n",
    "\n",
    "    def fit(self, tuple_ratings, rating_pairs_train):\n",
    "        # overall mean based on all training ratings\n",
    "        self.mean = np.mean([tuple_ratings[(u, m)] for (u, m) in rating_pairs_train])\n",
    "        # means for movies and users\n",
    "        if self.shrinkage:\n",
    "            mean_users = defaultdict(list)\n",
    "            mean_movies = defaultdict(list)\n",
    "            for (u, m) in rating_pairs_train:\n",
    "                mean_users[u].append(tuple_ratings[(u, m)])\n",
    "                mean_movies[m].append(tuple_ratings[(u, m)])\n",
    "            self.mean_users = {u: np.mean(mean_users[u])-self.mean for u in mean_users}\n",
    "            self.mean_movies = {m: np.mean(mean_movies[m])-self.mean for m in mean_movies}\n",
    "    \n",
    "    def predict(self, u, m):\n",
    "        \"\"\"\n",
    "        generate rating prediction for a user u and movie m\n",
    "        \"\"\"\n",
    "        rating = self.mean\n",
    "        if u in self.mean_users:\n",
    "            rating += self.shrinkage*self.mean_users[u]\n",
    "        if m in self.mean_movies:\n",
    "            rating += self.shrinkage*self.mean_movies[m]\n",
    "        return rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T00:50:10.485317Z",
     "start_time": "2018-03-31T00:38:04.864972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating train/test splits for scenario u'T1'\n",
      "got 6992764 training and 2996900 test ratings\n",
      "fitting model with shrinkage=0.0\n",
      "Scenario T1: RMSE: 1.06044; MAE: 0.85552\n",
      "fitting model with shrinkage=0.1\n",
      "Scenario T1: RMSE: 1.02330; MAE: 0.82574\n",
      "fitting model with shrinkage=0.5\n",
      "Scenario T1: RMSE: 0.91333; MAE: 0.71937\n",
      "fitting model with shrinkage=0.9\n",
      "Scenario T1: RMSE: 0.88074; MAE: 0.68157\n",
      "fitting model with shrinkage=1.0\n",
      "Scenario T1: RMSE: 0.88614; MAE: 0.68404\n",
      "generating train/test splits for scenario u'T2a'\n",
      "got 7064138 training and 2925526 test ratings\n",
      "fitting model with shrinkage=0.0\n",
      "Scenario T2a: RMSE: 1.05938; MAE: 0.85143\n",
      "fitting model with shrinkage=0.1\n",
      "Scenario T2a: RMSE: 1.04342; MAE: 0.83821\n",
      "fitting model with shrinkage=0.5\n",
      "Scenario T2a: RMSE: 0.99569; MAE: 0.79322\n",
      "fitting model with shrinkage=0.9\n",
      "Scenario T2a: RMSE: 0.97628; MAE: 0.76558\n",
      "fitting model with shrinkage=1.0\n",
      "Scenario T2a: RMSE: 0.97610; MAE: 0.76332\n",
      "generating train/test splits for scenario u'T2b'\n",
      "got 7015197 training and 2974467 test ratings\n",
      "fitting model with shrinkage=0.0\n",
      "Scenario T2b: RMSE: 1.05763; MAE: 0.85382\n",
      "fitting model with shrinkage=0.1\n",
      "Scenario T2b: RMSE: 1.03627; MAE: 0.83724\n",
      "fitting model with shrinkage=0.5\n",
      "Scenario T2b: RMSE: 0.97058; MAE: 0.77542\n",
      "fitting model with shrinkage=0.9\n",
      "Scenario T2b: RMSE: 0.94101; MAE: 0.73810\n",
      "fitting model with shrinkage=1.0\n",
      "Scenario T2b: RMSE: 0.93976; MAE: 0.73544\n",
      "generating train/test splits for scenario u'T3'\n",
      "got 6827065 training and 3162599 test ratings\n",
      "fitting model with shrinkage=0.0\n",
      "Scenario T3: RMSE: 1.05580; MAE: 0.85198\n",
      "fitting model with shrinkage=0.1\n",
      "Scenario T3: RMSE: 1.03854; MAE: 0.83850\n",
      "fitting model with shrinkage=0.5\n",
      "Scenario T3: RMSE: 0.98629; MAE: 0.78893\n",
      "fitting model with shrinkage=0.9\n",
      "Scenario T3: RMSE: 0.96390; MAE: 0.75888\n",
      "fitting model with shrinkage=1.0\n",
      "Scenario T3: RMSE: 0.96327; MAE: 0.75657\n"
     ]
    }
   ],
   "source": [
    "for scenario in [\"T1\", \"T2a\", \"T2b\", \"T3\"]:\n",
    "    # get train/test data\n",
    "    rating_pairs_train, rating_pairs_test, map_index2userid_train, map_index2movieid_train, map_userid2index_train, map_movieid2index_train = split_traintest(scenario)\n",
    "    for shrinkage in [0., 0.1, 0.5, 0.9, 1.]:\n",
    "        # initalize means model\n",
    "        mmodel = MeansModel(shrinkage)\n",
    "        print(\"fitting model with shrinkage=%.1f\" % shrinkage)\n",
    "        mmodel.fit(tuple_ratings, rating_pairs_train)\n",
    "        # get a vector with target ratings for test tuples\n",
    "        y_true = np.array([tuple_ratings[(u, m)] for (u, m) in rating_pairs_test])\n",
    "        # get the corresponding predictions\n",
    "        y_pred = np.array([mmodel.predict(u, m) for (u, m) in rating_pairs_test])\n",
    "        print(\"Scenario %s: RMSE: %.5f; MAE: %.5f\" % (scenario, np.sqrt(mean_squared_error(y_true, y_pred)), mean_absolute_error(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# matrix with pairwise data\n",
    "\n",
    "# left and right input vectors\n",
    "X1 = np.eye(n_input)\n",
    "X2 = np.eye(n_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1 = np.eye(n_input)\n",
    "X2 = np.eye(n_output)\n",
    "mses1 = []\n",
    "mses2 = []\n",
    "mses3 = []\n",
    "e_dims =  [2, 10, 25, 50, 75, 100, 250, 400, 500, 750, 1000]\n",
    "for e_dim in e_dims:\n",
    "    model = SimilarityEncoder(n_input, e_dim, n_output, opt=keras.optimizers.Adamax(lr=0.005 if e_dim <= 250 else l_rates[e_dim]))\n",
    "    model.fit(X1, A, epochs=50)\n",
    "    mse = msqe(A, model.predict(X1))\n",
    "    mses1.append(mse)\n",
    "    print \"mse with %4i e_dim: %.8f\" % (e_dim, mse)\n",
    "    Y1 = model.transform(X1)\n",
    "    model = SimilarityEncoder(n_output, e_dim, n_input, W_ll=Y1.T, opt=keras.optimizers.Adamax(lr=0.005 if e_dim <= 250 else l_rates[e_dim]))\n",
    "    model.fit(X2, A.T, epochs=50)\n",
    "    mse = msqe(A.T, model.predict(X2))\n",
    "    mses2.append(mse)\n",
    "    print \"mse with %4i e_dim: %.8f\" % (e_dim, mse)\n",
    "    Y2 = model.transform(X2)\n",
    "    # the dot product of both embeddings should also approximate A\n",
    "    mse = msqe(A, np.dot(Y1, Y2.T))\n",
    "    mses3.append(mse)\n",
    "for i, e_dim in enumerate(e_dims):\n",
    "    print \"mse with %4i e_dim: %.8f / %.8f / %.8f\" % (e_dim, mses1[i], mses2[i], mses3[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
